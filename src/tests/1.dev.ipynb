{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07f183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from  torch.nn.utils import spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b86de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 512, 8, 8])\n",
      "torch.Size([1, 512, 4, 4])\n",
      "torch.Size([1, 512, 2, 2])\n",
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config():\n",
    "    IMAGE_HEIGHT = 256\n",
    "    IMAGE_WIDTH = 256\n",
    "    BATCH_SIZE = 4\n",
    "    LATENT_DIM = 256\n",
    "    DEVICE = \"cpu\"\n",
    "cfg = Config()\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,channels_in,channels_out,with_norm=True):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        if with_norm:\n",
    "            self.block = nn.Sequential(\n",
    "                                        nn.Conv2d(in_channels=channels_in,out_channels=channels_out,\\\n",
    "                                                    kernel_size=3,stride=2,bias=False,padding=1),           \n",
    "                                        nn.InstanceNorm2d(channels_out),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                                        nn.Conv2d(in_channels=channels_in,out_channels=channels_out,\\\n",
    "                                                    kernel_size=3,stride=2,bias=False,padding=1),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "    def forward(self,x):\n",
    "        return self.block(x)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.block1 = EncoderBlock(3,64,with_norm=False)\n",
    "        self.block2 = EncoderBlock(64,128)\n",
    "        self.block3 = EncoderBlock(128,256)\n",
    "        self.block4 = EncoderBlock(256,512)\n",
    "        self.block5 = EncoderBlock(512,512)\n",
    "        self.block6 = EncoderBlock(512,512)\n",
    "        self.block7 = EncoderBlock(512,512)\n",
    "        self.flattening_block = nn.Conv2d(512,2048,kernel_size=1,padding=0)\n",
    "\n",
    "        self.linear_mu_branch = nn.Linear(in_features=2048,out_features=cfg.LATENT_DIM)\n",
    "        self.linear_var_branch = nn.Linear(in_features=2048,out_features=cfg.LATENT_DIM)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.block1(x)\n",
    "        print(x.shape)\n",
    "        x = self.block2(x)\n",
    "        print(x.shape)\n",
    "        x = self.block3(x)\n",
    "        print(x.shape)\n",
    "        x = self.block4(x)\n",
    "        print(x.shape)\n",
    "        x = self.block5(x)\n",
    "        print(x.shape)\n",
    "        x = self.block6(x)\n",
    "        print(x.shape)\n",
    "        x = self.block7(x)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(x.shape[0],x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        print(x.shape)\n",
    "        \n",
    "        \n",
    "        mu = self.linear_mu_branch(x)\n",
    "        var = self.linear_var_branch(x)\n",
    "        \n",
    "        print(mu.shape,var.shape)\n",
    "        return mu,var\n",
    "        # return x\n",
    "    def get_latent_vector(self,mu,var):\n",
    "        epsilon = torch.randn(mu.size(),device=cfg.DEVICE)\n",
    "        latent_vec = mu  + torch.exp((var*0.5)) * epsilon  \n",
    "        return latent_vec\n",
    "\n",
    "\n",
    "img = torch.randn(1,3,256,256)\n",
    "# print(img.shape)\n",
    "enc = Encoder()\n",
    "mean,variance  = enc(img)\n",
    "enc.get_latent_vector(mean,variance).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7102d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(nn.Module):\n",
    "    def __init__(self,num_channels):\n",
    "        super(SPADE, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_channels,affine=False)\n",
    "        self.conv_1 = nn.Sequential(spectral_norm(nn.Conv2d(num_channels,128,kernel_size=3,padding=1)),\\\n",
    "                                   nn.ReLU())\n",
    "        self.conv_1_1  = spectral_norm(nn.Conv2d(128, num_channels, kernel_size=3, padding=1))\n",
    "        self.conv_2 = spectral_norm(nn.Conv2d(128,  num_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "    def forward(self,x,segmentation_map):\n",
    "        # print(x.shape)\n",
    "        # BN\n",
    "        x = self.bn(x)\n",
    "        # Resize Map\n",
    "        segmentation_map = F.interpolate(segmentation_map, size=x.size()[2:], mode='nearest')\n",
    "        # Calc gamma and beta \n",
    "        output_shared = self.conv_1(x)\n",
    "        gamma = self.conv_1_1(output_shared)\n",
    "        beta = self.conv_2(output_shared)\n",
    "        # rescale\n",
    "        # print(x.shape,gamma.shape,beta.shape)\n",
    "        out = x*(1+gamma) + beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8010f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spade , relu ,conv, spade, relu, conv\n",
    "# skip should also be a spade,conv block\n",
    "class SPADEResBlk(nn.Module):\n",
    "    def __init__(self,num_features_in,num_features_out):\n",
    "        super(SPADEResBlk,self,).__init__()\n",
    "        self.spade1 = SPADE(num_channels=num_features_in)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels=num_features_in,\\\n",
    "            out_channels=num_features_out,kernel_size=3,padding=1))\n",
    "        self.spade2 = SPADE(num_channels=num_features_out)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(in_channels=num_features_out,\\\n",
    "            out_channels=num_features_out,kernel_size=3,padding=1))\n",
    "        self.skip_connection_spade = SPADE(num_channels=num_features_in)\n",
    "        self.skip_connection_conv = spectral_norm(nn.Conv2d(in_channels=num_features_in,\\\n",
    "                                                out_channels=num_features_out,\\\n",
    "                                                    kernel_size=1,\\\n",
    "                                                        bias=False))\n",
    "    \n",
    "    def forward(self,x,segmentation_map):\n",
    "        skip_features = self.skip_connection_spade(x,segmentation_map)\n",
    "        skip_features = F.leaky_relu(skip_features)\n",
    "        skip_features = self.skip_connection_conv(skip_features)\n",
    "\n",
    "        x = self.conv1(F.leaky_relu(self.spade1(x,segmentation_map)))\n",
    "        x = self.conv2(F.leaky_relu(self.spade2(x,segmentation_map)))\n",
    "        return skip_features + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3ef83608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.linear1 = nn.Linear(cfg.LATENT_DIM,16384)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.block1 = SPADEResBlk(num_features_in=1024,num_features_out=1024)\n",
    "        self.block2 = SPADEResBlk(num_features_in=1024,num_features_out=1024)\n",
    "        self.block3 = SPADEResBlk(num_features_in=1024,num_features_out=512)\n",
    "        self.block4 = SPADEResBlk(num_features_in=512,num_features_out=256)\n",
    "        self.block5 = SPADEResBlk(num_features_in=256,num_features_out=128)\n",
    "        self.block6 = SPADEResBlk(num_features_in=128,num_features_out=64)\n",
    "        self.block7 = SPADEResBlk(num_features_in=64,num_features_out=32)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,padding=1)\n",
    "\n",
    "    def forward(self,latent_vec,segmentation_map):\n",
    "        x = self.linear1(latent_vec)\n",
    "        x = x.reshape(-1,1024,4,4)\n",
    "        x = self.block1(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        x = self.block2(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        x = self.block3(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        x = self.block4(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        x = self.block5(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        x = self.block6(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        print(x.shape)        \n",
    "        x = self.block7(x,segmentation_map)\n",
    "        x = self.upsample(x)\n",
    "        print(x.shape)        \n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d8d2fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noufal.samsudin\\Anaconda3\\envs\\ptorchenv\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator().to(\"cpu\")\n",
    "segmentation_map = torch.randn(1,256,256,10).to(\"cpu\")\n",
    "lvec = torch.randn(1,256).to(\"cpu\")\n",
    "op = gen(lvec,segmentation_map)\n",
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "589538d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 128, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imga = torch.randn(2,3,128,128)\n",
    "imgb = torch.randn(2,3,128,128)\n",
    "\n",
    "torch.concat([imga,imgb],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "340c3089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 128, 127, 127]),\n",
       " torch.Size([5, 256, 63, 63]),\n",
       " torch.Size([5, 512, 31, 31]),\n",
       " torch.Size([5, 512, 15, 15]),\n",
       " torch.Size([5])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DescriminatorBlock(nn.Module):\n",
    "    def __init__(self,channels_in,channels_out,with_norm=True):\n",
    "        super(DescriminatorBlock,self).__init__()\n",
    "        if with_norm:\n",
    "            self.block = nn.Sequential(\n",
    "                                        spectral_norm(nn.Conv2d(in_channels=channels_in,out_channels=channels_out,\\\n",
    "                                                    kernel_size=4,stride=2,bias=False,padding=1)),           \n",
    "                                        nn.InstanceNorm2d(channels_out),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                                        spectral_norm(nn.Conv2d(in_channels=channels_in,out_channels=channels_out,\\\n",
    "                                                    kernel_size=4,stride=2,bias=False,padding=1)),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "    def forward(self,x):\n",
    "        return self.block(x)\n",
    "class Descriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Descriminator,self).__init__()\n",
    "        # change input channeldim        \n",
    "        self.block1 = spectral_norm(nn.Conv2d(10,64,kernel_size=4,stride=2,bias=True))\n",
    "        self.block2 = DescriminatorBlock(64,128,False)\n",
    "        self.block3 = DescriminatorBlock(128,256)\n",
    "        self.block4 = DescriminatorBlock(256,512)\n",
    "        self.block5 = DescriminatorBlock(512,512)\n",
    "        self.in7 = nn.InstanceNorm2d(512)\n",
    "        self.conv8 = spectral_norm(nn.Conv2d(512,1,kernel_size=4))\n",
    "    \n",
    "    def forward(self,segmentation_map,img):\n",
    "        concat_img = torch.concat([segmentation_map,img],dim=1)\n",
    "        op1 = self.block2(self.block1(concat_img))\n",
    "        op2 = self.block3(op1)\n",
    "        op3 = self.block4(op2)\n",
    "        # print(op3.shape)\n",
    "        op4 = self.block5(op3)\n",
    "        op5 = self.conv8(F.leaky_relu(self.in7(op4))).mean(dim=(1,2,3))\n",
    "        return [op1,op2,op3,op4,op5]\n",
    "\n",
    "desc = Descriminator()\n",
    "imgA = torch.randn(5,7,512,512)\n",
    "imgB = torch.randn(5,3,512,512)\n",
    "op = desc(imgA,imgB)\n",
    "[t.shape for t in op]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "47497081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen loss = g_loss + kl_loss + vgg_loss + feature_loss\n",
    "# g loss between - loss between descriminator prediction, and actual label\n",
    "# kl_loss : encoder output mean,variance\n",
    "# vgg_loss: loss between generated image (by generator) and actual image\n",
    "# feature_loss: loss between real desc output and fake desc output\n",
    "# def gen_loss(pred,target):\n",
    "#     loss = F.binary_cross_entropy_with_logits(pred,target)\n",
    "#     return loss\n",
    "class Gen_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gen_loss,self).__init__()\n",
    "        self.criterion = F.binary_cross_entropy\n",
    "    def forward(self,pred,target):\n",
    "        return self.criterion(pred,target)\n",
    "# def kl_loss( mu, logvar):\n",
    "#     return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "class KLD_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLD_Loss,self).__init__()\n",
    "    def forward(self,mu,logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "# def vgg_loss ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7601a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\noufal.samsudin/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [06:03<00:00, 1.58MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "vgg = models.vgg19(pretrained=True)\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eb8de5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "avgpool\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for i, v in vgg.named_children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd573b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg19(pretrained=True).features\n",
    "f5 = nn.Sequential(*[vgg[x] for x in range(30)])\n",
    "img = torch.randn(1,3,256,256)\n",
    "f5(img).shape\n",
    "# for param in vgg.parameters():\n",
    "#     param.requires_grad = False\n",
    "#     print(param.name)\n",
    "i=0\n",
    "for _ in vgg.parameters():\n",
    "    i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f997c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8025)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss,self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).to(cfg.DEVICE).features\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.f1 = nn.Sequential(*[vgg[x] for x in range(2)])\n",
    "        self.f2 = nn.Sequential(*[vgg[x] for x in range(7)])\n",
    "        self.f3 = nn.Sequential(*[vgg[x] for x in range(12)])\n",
    "        self.f4 = nn.Sequential(*[vgg[x] for x in range(21)])\n",
    "        self.f5 = nn.Sequential(*[vgg[x] for x in range(30)])\n",
    "    def forward(self,x,y):\n",
    "        # print(x_input.shape,x_input.shape)\n",
    "        loss=0\n",
    "\n",
    "        x1 = self.f1(x)\n",
    "        y1 = self.f1(y)\n",
    "        # print(x.shape,y.shape)\n",
    "        # print(x1.shape,y1.shape)\n",
    "        loss1 = F.l1_loss(x1,y1)\n",
    "        # print(loss1)\n",
    "\n",
    "        x2 = self.f2(x)\n",
    "        y2 = self.f2(y)\n",
    "        loss2 = F.l1_loss(x2,y2)\n",
    "\n",
    "        x3 = self.f3(x)\n",
    "        y3 = self.f3(y)\n",
    "        loss3 = F.l1_loss(x3,y3)\n",
    "\n",
    "        x4 = self.f4(x)\n",
    "        y4 = self.f4(y)\n",
    "        loss4 = F.l1_loss(x4,y4)\n",
    "\n",
    "        x5 = self.f5(x)\n",
    "        y5 = self.f5(y)\n",
    "        loss5 = F.l1_loss(x5,y5)\n",
    "\n",
    "        loss += loss1/32 + loss2/16 + loss3/8 + loss4/4 + loss5\n",
    "        print(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "vggloss = VGGLoss()\n",
    "img1 = torch.randn(2,3,256,256)\n",
    "img2 = torch.randn(2,3,256,256)\n",
    "vggloss(img1,img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5e8b1ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2813, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_loss_disc(real_disc_outputs,fake_disc_outputs):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for real_disc_output,fake_disc_output in zip(real_disc_outputs,fake_disc_outputs):\n",
    "            for r_disc_output_feature,f_disc_output_feature in zip(real_disc_output,fake_disc_output):\n",
    "                loss+= F.l1_loss(r_disc_output_feature,f_disc_output_feature)\n",
    "        return loss/len(real_disc_outputs)\n",
    "class FeatureLossDisc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureLossDisc,self).__init__()\n",
    "    def forward(self,real_disc_outputs,fake_disc_outputs):\n",
    "        loss=0\n",
    "        for real_disc_output,fake_disc_output in zip(real_disc_outputs,fake_disc_outputs):\n",
    "            loss+= F.l1_loss(real_disc_output,fake_disc_output)\n",
    "        return loss/len(real_disc_outputs)\n",
    "\n",
    "imgA = torch.randn(5,7,256,256)\n",
    "imgB = torch.randn(5,3,256,256)\n",
    "imgC = torch.randn(5,3,256,256)\n",
    "op_real = desc(imgA,imgB)\n",
    "op_fake = desc(imgA,imgC)\n",
    "FeatureLossDisc()(op_real,op_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "93b596eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4656, 0.3340, 0.4195, 0.3598, 0.6264], grad_fn=<MeanBackward1>),\n",
       " tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "op_real[-1],torch.ones_like(op_real[-1]),torch.zeros_like(op_real[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "98700c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4411, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge_loss(op_real[-1],torch.ones_like(op_real[-1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb2a37f0bf35d3da73736950605ba39c9d88bf7386ed6e66874282d71ea1792e"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ptorchenv] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
